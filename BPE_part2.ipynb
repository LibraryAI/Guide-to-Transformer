{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding\n",
    "==================\n",
    "'Neural Machine Translation of Rare Words with Subword Units' by Rico Sennrich, Barry Haddow, Alexandria Burch 논문 참고\n",
    "<br>\n",
    "<br>\n",
    "### BPE 적용\n",
    "OpenNMT 에서는 BPE 적용에 glossary set, vocabulary set 등을 함께 이용한다. glossary set은 task-specific하게 적용하는 등 부가적인 요소이며, vocabulary set은 보통 training data의 word dictionary를 이용해 만든다. 'Attention is all you need!' 논문의 경우 40,000개의 BPE set과 같은 크기의 vocab set을 이용했으며, 같은 계열의 최신 논문인 'Improving Language Understanding by Generative Pre-Training' 역시 같은 confguration을 사용했다.\n",
    "<br>\n",
    "<br>\n",
    "BPE을 새로운 데이터셋에 적용하는 과정을 자세하게 설명해보려 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE 적용 코드 설명\n",
    "OpenNMT의 apply_bpe는 ①segment -> ②_isolate_glossaries -> ③isolate_glossaries -> ④encode -> ⑤get_pairs -> ⑥check_vocab_and_split -> ⑦recursive_split 의 함수 호출 과정을 거치며 input sentence를 tokenize한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def segment(self, sentence):\n",
    "        \"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n",
    "        output = []\n",
    "        for word in sentence.split():\n",
    "            new_word = [out for segment in self._isolate_glossaries(word)\n",
    "                        for out in encode(segment,\n",
    "                                          self.bpe_codes,\n",
    "                                          self.bpe_codes_reverse,\n",
    "                                          self.vocab,\n",
    "                                          self.separator,\n",
    "                                          self.version,\n",
    "                                          self.cache,\n",
    "                                          self.glossaries)]\n",
    "\n",
    "            for item in new_word[:-1]:\n",
    "                output.append(item + self.separator)\n",
    "            output.append(new_word[-1])\n",
    "\n",
    "        return ' '.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__segment(sentence)__<br>\n",
    "_ex: sentence= \"긴 하루였다\" -> \"긴 하루@@ 였@ 다\"_<br>\n",
    "_input: sentence; whitespace-토큰화된 스트링_<br>\n",
    "_output: BPE 로직에 의해 분해된 요소들에는 separator가 요소 뒤에 붙은 상태에서 1 whitespace로 구분되는 하나의 스트링_<br>\n",
    "<br>\n",
    "예시에서 볼 수 있듯이 원 문장에서 떨어져 있던 단어 사이에는 새로운 구분자가 없다.segment 함수는 BPE를 수행하는 전체과정을 포함하는 함수이다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _isolate_glossaries(self, word):\n",
    "\n",
    "        word_segments = [word]\n",
    "        for gloss in self.glossaries:\n",
    "            word_segments = [out_segments for segment in word_segments\n",
    "                             for out_segments in isolate_glossary(segment, gloss)]\n",
    "        return word_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___isolate_glossaries(word)__<br>\n",
    "_ex: glossaries = [땅콩, 비행], word = \"비행기땅콩먹는비행기땅콩비행기?\" -> [\"비행\", \"기\", \"땅콩\", \"먹는\", \"비행\", \"기\", \"땅콩\", \"비행\", \"기?\"]_<br>\n",
    "_input: word; 단어 스트링_<br>\n",
    "_output: glossaries 리스트에 있는 모든 glossary 들을 기준으로 split된 segment 리스트_<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_glossary(word, glossary):\n",
    "    \"\"\"\n",
    "    Isolate a glossary present inside a word.\n",
    "    Returns a list of subwords. In which all 'glossary' glossaries are isolated \n",
    "    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n",
    "        ['1934', 'USA', 'B', 'USA']\n",
    "    \"\"\"\n",
    "    if word == glossary or glossary not in word:\n",
    "        return [word]\n",
    "    else:\n",
    "        splits = word.split(glossary)\n",
    "        segments = [segment.strip() for split in splits[:-1]\n",
    "                    for segment in [split, glossary] if segment != '']\n",
    "        return segments + [splits[-1].strip()] if splits[-1] != '' else segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__isolate_glossaries(word, glossary)__<br>\n",
    "_ex: glossary = 땅콩, segment = \"땅콩버터먹고땅콩먹자\" -> [\"땅콩\", \"버터먹고\", \"땅콩\", \"먹자\"]_<br>\n",
    "_input: segment, glossary; 스트링, 스트링_<br>\n",
    "_output: segments; glossary를 기준으로 split된 segment 리스트_<br>\n",
    "\n",
    "glossary 스트링이 segment 스트링에 포함된 경우, glossary를 기준으로 split을 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries=None):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n",
    "    \"\"\"\n",
    "\n",
    "    # encode는 BPE를 적용하는 함수다. glossary가 주어진 경우, glossary를 기준으로 분해된 segment를 인풋으로 받아 다음의 과정을 수행한다.\n",
    "    # 1) segment 스트링을 튜플로 분해한 후, get_pairs 함수를 통해 해당 segment 캐릭터들의 pair집합을 구한다\n",
    "    # 2) pair집합과 bpe_codes 간에 교집합이 없을 때 까지 다음을 수행한다\n",
    "    # 2-1) pair 묶음들과 bpe_codes의 겹치는 원소 중 가장 처음 원소(빈도수가 가장 많은)를 bigram에 할당\n",
    "    # 2-2) 1의 튜플을 bigram에 해당하는 스트링의 원소가 있는 인덱스 까지의 원소들을 하나의 스트링으로 만들어 new_word 리스트에 넣고, bigram을 append한 후,  다음 인덱스 부터 2-2를 다시 반복한다\n",
    "    # 2-3) 2-2에서 구해진 new_word 리스트를 튜플로 변환하고 get_pairs를 통해 pair 집합을 구한다.\n",
    "    # 3) Vocab가 주어졌다면, 2의 과정을 통해 Byte-pair로 뭉쳐진 토큰들을 check_vocab_and_split을 통해 해당 토큰들이 vocab 집합에 속해있는지 확인 후 OOV들은 잘게 쪼갠다\n",
    "    # 4) Vocabulary와 잘게 쪼개진 OOV 스트링으로 이루어진 리스트를 반환한다\n",
    "\n",
    "    if orig in cache:\n",
    "        return cache[orig]\n",
    "\n",
    "    if orig in glossaries:\n",
    "        cache[orig] = (orig,)\n",
    "        return (orig,)\n",
    "\n",
    "    word = tuple(orig[:-1]) + (orig[-1] + '</w>',)\n",
    "    \n",
    "    pairs = get_pairs(word)\n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    while True:\n",
    "        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        if bigram not in bpe_codes:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>', ''),)\n",
    "\n",
    "    if vocab:\n",
    "        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n",
    "\n",
    "    cache[orig] = word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode는 BPE를 적용하는 함수다. glossary가 주어진 경우, glossary를 기준으로 분해된 segment를 인풋으로 받아 다음의 과정을 수행한다.<br>\n",
    "1. segment 스트링을 튜플로 분해한 후, get_pairs 함수를 통해 해당 segment 캐릭터들의 pair집합을 구한다\n",
    "2. pair집합과 bpe_codes 간에 교집합이 없을 때 까지 다음을 수행한다\n",
    "    1. pair 묶음들과 bpe_codes의 겹치는 원소 중 가장 처음 원소(빈도수가 가장 많은)를 bigram에 할당\n",
    "    2. 1의 튜플을 bigram에 해당하는 스트링의 원소가 있는 인덱스 까지의 원소들을 하나의 스트링으로 만들어 new_word 리스트에 넣고, bigram을 append한 후,  다음 인덱스 부터 B를 다시 반복한다\n",
    "    3. B에서 구해진 new_word 리스트를 튜플로 변환하고 get_pairs를 통해 pair 집합을 구한다.\n",
    "3. Vocab가 주어졌다면, 2의 과정을 통해 Byte-pair로 뭉쳐진 토큰들을 check_vocab_and_split을 통해 해당 토큰들이 vocab 집합에 속해있는지 확인 후 OOV들은 잘게 쪼갠다\n",
    "4. Vocabulary와 잘게 쪼개진 OOV 스트링으로 이루어진 리스트를 반환한다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get_pairs(word)__<br>\n",
    "_ex: word = [\"20, \"018\", \"년\", \"가\", \"즈\"] -> {(\"20\", \"018\"), (\"018\", \"년\"), (\"년\", \"가\"), (\"가\", \"즈\")}_<br>\n",
    "_input: word; 스트링 리스트_<br>\n",
    "_output: pair 집합_<br>\n",
    "<br>\n",
    "인풋 리스트 내의 인접한 두 스트링의 pair를 원소로 하는 집합 구하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocab_and_split(orig, bpe_codes, vocab, separator):\n",
    "    \"\"\"Check for each segment in word if it is in-vocabulary,\n",
    "    and segment OOV segments into smaller units by reversing the BPE merge operations\"\"\"\n",
    "\n",
    "    # orig; 튜플\n",
    "\n",
    "    # orig 튜플 속의 원소들이 vocab 집합에 속했는지 확인 후 vocab에 속할 때 까지 recursive_split을 통해 잘게 쪼개 OOV(Out of Vcoabulary)라면 잘게 쪼갠다.\n",
    "    \n",
    "    out = []\n",
    "\n",
    "    for segment in orig[:-1]:\n",
    "        if segment + separator in vocab:\n",
    "            out.append(segment)\n",
    "        else:\n",
    "            #sys.stderr.write('OOV: {0}\\n'.format(segment))\n",
    "            for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n",
    "                out.append(item)\n",
    "\n",
    "    segment = orig[-1]\n",
    "    if segment in vocab:\n",
    "        out.append(segment)\n",
    "    else:\n",
    "        #sys.stderr.write('OOV: {0}\\n'.format(segment))\n",
    "        for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n",
    "            out.append(item)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__check_vocab_and_split(orig, bpe_codes, vocab, separator)__<br>\n",
    "_input: orig; 튜플_<br>\n",
    "_output: Vocabulary와 잘게 쪼개진 OOV 스트링으로 이루어진 리스트_<br>\n",
    "<br>\n",
    "orig 튜플 속의 원소들이 vocab 집합에 속했는지 확인 후 vocab에 속할 때 까지 recursive_split을 통해 잘게 쪼개 OOV(Out of Vcoabulary)라면 잘게 쪼갠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_split(segment, bpe_codes, vocab, separator, final=False):\n",
    "    \"\"\"Recursively split segment into smaller units (by reversing BPE merges)\n",
    "    until all units are either in-vocabulary, or cannot be split futher.\"\"\"\n",
    "\n",
    "    # segment; 스트링\n",
    "\n",
    "    # 조건부에 일치하는 스트링을 yield로 반환\n",
    "    # ex. bpe_codes= {'는다</w>': ('는', '다</w>')}, vocab= {'는', '다'}, segment= '먹는다', \n",
    "    #     output = '먹는다'\n",
    "    try:\n",
    "        if final:\n",
    "            left, right = bpe_codes[segment + '</w>']\n",
    "            right = right[:-4]\n",
    "        else:\n",
    "            left, right = bpe_codes[segment]\n",
    "    except:\n",
    "        #sys.stderr.write('cannot split {0} further.\\n'.format(segment))\n",
    "        yield segment\n",
    "        return\n",
    "\n",
    "    if left + separator in vocab:\n",
    "        yield left\n",
    "    else:\n",
    "        for item in recursive_split(left, bpe_codes, vocab, separator, False):\n",
    "            yield item\n",
    "\n",
    "    if (final and right in vocab) or (not final and right + separator in vocab):\n",
    "        yield right\n",
    "    else:\n",
    "        for item in recursive_split(right, bpe_codes, vocab, separator, final):\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__recursive_split(segment, bpe_codes, vocab, separator, final=False)__<br>\n",
    "_ex: bpecodes= {'는다</w>': ('는', '다</w>')}, vocab= {'는', '다'}, segment= '먹는다'-> '먹는다' (bpe 집합의 key에 매칭되지 않는 토큰의 경우_<br>\n",
    "_ex: bpecodes= {'는다</w>': ('는', '다</w>')}, vocab= {'는', '다</w>'}, segment= '는다'-> '는', '다</w>'_<br>\n",
    "_input: segment; 스트링_<br>\n",
    "_output: 조건부에 일치하는 스트링을 yield로 반환_<br>\n",
    "<br>\n",
    "recursive_split은 인풋 segment에 BPE 오퍼레이션을 역으로 수행해 점점 분해해가며 vocab 집합에 속해 있거나, 더 잘게 쪼갤 수 없을 때 까지 반복하는 함수이다. bpe 집합의 key와 비교해 bpe에 있는 토큰이라면 bpe 오퍼레이션을 역으로 수행해 분해한다. 그 후, 역 bpe에 의해 분해된 토큰이 vocab 집합에 속한 원소인지 확인 후 해당된다면 반환하고, 원소가 아니라면 recursive_split을 반복적으로 수행하는 재귀함수.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
